import time

import framework as fw
import numpy as np

# test check generated by claude-4-sonnet

def gradient_check(f, *args, tol=1e-6, backward=False, test_name="", **kwargs):
    """
    Enhanced gradient check with detailed output
    """
    print(f"  [GRADIENT CHECK] Running gradient check for: {test_name}")
    start_time = time.time()
    
    eps = 1e-4
    numerical_grads = [np.zeros(a.shape) for a in args]
    
    print(f"    [COMPUTE] Computing numerical gradients for {len(args)} tensors...")
    for i in range(len(args)):
        tensor_shape = args[i].shape
        tensor_size = args[i].realize_cached_data().size
        print(f"      Tensor {i+1}: shape {tensor_shape}, size {tensor_size}")
        
        for j in range(tensor_size):
            args[i].realize_cached_data().flat[j] += eps
            f1 = float(f(*args, **kwargs).numpy().sum())
            args[i].realize_cached_data().flat[j] -= 2 * eps
            f2 = float(f(*args, **kwargs).numpy().sum())
            args[i].realize_cached_data().flat[j] += eps
            numerical_grads[i].flat[j] = (f1 - f2) / (2 * eps)
    
    print(f"    [COMPUTE] Computing analytical gradients...")
    if not backward:
        out = f(*args, **kwargs)
        computed_grads = [x.numpy() for x in out.op.gradient_as_tuple(fw.Tensor(np.ones(out.shape)), out)]
    else:
        out = f(*args, **kwargs).sum()
        out.backward()
        computed_grads = [a.grad.numpy() for a in args]
    
    # Compute errors for each tensor
    errors = []
    for i in range(len(args)):
        error = np.linalg.norm(computed_grads[i] - numerical_grads[i])
        errors.append(error)
        print(f"    [ERROR] Tensor {i+1} gradient error: {error:.2e}")
    
    total_error = sum(errors)
    elapsed_time = time.time() - start_time
    
    print(f"    [RESULT] Total gradient error: {total_error:.2e} (tolerance: {tol:.2e})")
    print(f"    [TIME] Time elapsed: {elapsed_time:.3f} seconds")
    
    if total_error < tol:
        print(f"    [PASSED] {test_name}")
    else:
        print(f"    [FAILED] {test_name}")
        
    assert total_error < tol
    return computed_grads


def test_topo_sort():
    """
    Test topological sorting of computation graph
    """
    print("\n[TEST] Testing Topological Sort")
    print("=" * 50)
    
    # Test case 1
    print("\n[CASE 1] Simple arithmetic operations")
    a1, b1 = fw.Tensor(np.asarray([[0.88282157]])), fw.Tensor(np.asarray([[0.90170084]]))
    print(f"  Input a1: {a1.numpy()}")
    print(f"  Input b1: {b1.numpy()}")
    
    c1 = 3 * a1 * a1 + 4 * b1 * a1 - a1
    print(f"  Expression: 3 * a1 * a1 + 4 * b1 * a1 - a1")
    print(f"  Result: {c1.numpy()}")

    soln = np.array(
        [
            np.array([[0.88282157]]),
            np.array([[2.64846471]]),
            np.array([[2.33812177]]),
            np.array([[0.90170084]]),
            np.array([[3.60680336]]),
            np.array([[3.1841638]]),
            np.array([[5.52228558]]),
            np.array([[-0.88282157]]),
            np.array([[4.63946401]]),
        ]
    )

    topo_order = np.array([x.numpy() for x in fw.autograd.find_topo_sort([c1])])
    print(f"  [INFO] Expected {len(soln)} nodes in topological order")
    print(f"  [INFO] Found {len(topo_order)} nodes in topological order")

    assert len(soln) == len(topo_order)
    np.testing.assert_allclose(topo_order, soln, rtol=1e-06, atol=1e-06)
    print("  [PASSED] Test Case 1")

    # Test case 2
    print("\n[CASE 2] Matrix operations with broadcasting")
    a1, b1 = fw.Tensor(np.asarray([[0.20914675], [0.65264178]])), fw.Tensor(
        np.asarray([[0.65394286, 0.08218317]])
    )
    print(f"  Input a1 shape: {a1.shape}")
    print(f"  Input b1 shape: {b1.shape}")
    
    c1 = 3 * ((b1 @ a1) + (2.3412 * b1) @ a1) + 1.5
    print(f"  Expression: 3 * ((b1 @ a1) + (2.3412 * b1) @ a1) + 1.5")
    print(f"  Result: {c1.numpy()}")

    soln = [
        np.array([[0.65394286, 0.08218317]]),
        np.array([[0.20914675], [0.65264178]]),
        np.array([[0.19040619]]),
        np.array([[1.53101102, 0.19240724]]),
        np.array([[0.44577898]]),
        np.array([[0.63618518]]),
        np.array([[1.90855553]]),
        np.array([[3.40855553]]),
    ]

    topo_order = [x.numpy() for x in fw.autograd.find_topo_sort([c1])]
    print(f"  [INFO] Expected {len(soln)} nodes in topological order")
    print(f"  [INFO] Found {len(topo_order)} nodes in topological order")

    assert len(soln) == len(topo_order)
    # step through list as entries differ in length
    for t, s in zip(topo_order, soln):
        np.testing.assert_allclose(t, s, rtol=1e-06, atol=1e-06)
    print("  [PASSED] Test Case 2")

    # Test case 3
    print("\n[CASE 3] Complex matrix operations")
    a = fw.Tensor(np.asarray([[1.4335016, 0.30559972], [0.08130171, -1.15072371]]))
    b = fw.Tensor(np.asarray([[1.34571691, -0.95584433], [-0.99428573, -0.04017499]]))
    print(f"  Input a shape: {a.shape}")
    print(f"  Input b shape: {b.shape}")
    
    e = (a @ b + b - a) @ a
    print(f"  Expression: (a @ b + b - a) @ a")
    print(f"  Result shape: {e.shape}")

    topo_order = np.array([x.numpy() for x in fw.autograd.find_topo_sort([e])])

    soln = np.array(
        [
            np.array([[1.4335016, 0.30559972], [0.08130171, -1.15072371]]),
            np.array([[1.34571691, -0.95584433], [-0.99428573, -0.04017499]]),
            np.array([[1.6252339, -1.38248184], [1.25355725, -0.03148146]]),
            np.array([[2.97095081, -2.33832617], [0.25927152, -0.07165645]]),
            np.array([[-1.4335016, -0.30559972], [-0.08130171, 1.15072371]]),
            np.array([[1.53744921, -2.64392589], [0.17796981, 1.07906726]]),
            np.array([[1.98898021, 3.51227226], [0.34285002, -1.18732075]]),
        ]
    )

    print(f"  [INFO] Expected {len(soln)} nodes in topological order")
    print(f"  [INFO] Found {len(topo_order)} nodes in topological order")
    
    assert len(soln) == len(topo_order)
    np.testing.assert_allclose(topo_order, soln, rtol=1e-06, atol=1e-06)
    print("  [PASSED] Test Case 3")
    
    print("\n[SUCCESS] All Topological Sort Tests PASSED!")


def test_gradient():
    """
    Test gradient computation with various operations
    """
    print("\n[TEST] Testing Gradient Computation")
    print("=" * 50)
    
    print("\n[BASIC] Basic Gradient Tests")
    
    # Test 1: Matrix operations with summation
    gradient_check(
        lambda A, B, C: fw.summation((A @ B + C) * (A @ B), axes=None),
        fw.Tensor(np.random.randn(10, 9)),
        fw.Tensor(np.random.randn(9, 8)),
        fw.Tensor(np.random.randn(10, 8)),
        backward=True,
        test_name="Matrix multiplication and element-wise operations"
    )
    
    # Test 2: Broadcasting operations
    gradient_check(
        lambda A, B: fw.summation(fw.broadcast_to(A, shape=(10, 9)) * B, axes=None),
        fw.Tensor(np.random.randn(10, 1)),
        fw.Tensor(np.random.randn(10, 9)),
        backward=True,
        test_name="Broadcasting and element-wise multiplication"
    )
    
    # Test 3: Reshape and division operations
    gradient_check(
        lambda A, B, C: fw.summation(fw.reshape(A, shape=(10, 10)) @ B / 5 + C, axes=None),
        fw.Tensor(np.random.randn(100)),
        fw.Tensor(np.random.randn(10, 5)),
        fw.Tensor(np.random.randn(10, 5)),
        backward=True,
        test_name="Reshape, matrix multiplication, and division"
    )

    # print("\n[HIGHER-ORDER] Higher-Order Gradient Test")
    # print("  Testing gradient of gradient (second-order derivatives)")
    
    # # check gradient of gradient
    # x2 = fw.Tensor([6])
    # x3 = fw.Tensor([0])
    # print(f"  Input x2: {x2.numpy()}")
    # print(f"  Input x3: {x3.numpy()}")
    
    # y = x2 * x2 + x2 * x3
    # print(f"  Expression: y = x2 * x2 + x2 * x3")
    # print(f"  Forward result y: {y.numpy()}")
    
    # y.backward()
    # grad_x2 = x2.grad
    # grad_x3 = x3.grad
    # print(f"  First-order gradient dy/dx2: {grad_x2.numpy()}")
    # print(f"  First-order gradient dy/dx3: {grad_x3.numpy()}")
    
    # # gradient of gradient
    # grad_x2.backward()
    # grad_x2_x2 = x2.grad
    # grad_x2_x3 = x3.grad
    # print(f"  Second-order gradient d²y/dx2²: {grad_x2_x2.numpy()}")
    # print(f"  Second-order gradient d²y/dx2dx3: {grad_x2_x3.numpy()}")
    
    # x2_val = x2.numpy()
    # x3_val = x3.numpy()
    
    # # Verify results
    # expected_y = x2_val * x2_val + x2_val * x3_val
    # expected_grad_x2 = 2 * x2_val + x3_val
    # expected_grad_x3 = x2_val
    # expected_grad_x2_x2 = 2
    # expected_grad_x2_x3 = 1
    
    # print(f"  [CHECK] Forward pass: {y.numpy()} == {expected_y}")
    # print(f"  [CHECK] dy/dx2: {grad_x2.numpy()} == {expected_grad_x2}")
    # print(f"  [CHECK] dy/dx3: {grad_x3.numpy()} == {expected_grad_x3}")
    # print(f"  [CHECK] d²y/dx2²: {grad_x2_x2.numpy()} == {expected_grad_x2_x2}")
    # print(f"  [CHECK] d²y/dx2dx3: {grad_x2_x3.numpy()} == {expected_grad_x2_x3}")
    
    # assert y.numpy() == expected_y
    # assert grad_x2.numpy() == expected_grad_x2
    # assert grad_x3.numpy() == expected_grad_x3
    # assert grad_x2_x2.numpy() == expected_grad_x2_x2
    # assert grad_x2_x3.numpy() == expected_grad_x2_x3
    
    # print("  [PASSED] Higher-order gradient test")
    print("\n[SUCCESS] All Gradient Tests PASSED!")


if __name__ == "__main__":
    print("[START] Starting Autograd Test Suite")
    print("=" * 60)
    
    start_time = time.time()
    
    try:
        test_topo_sort()
        test_gradient()
        
        total_time = time.time() - start_time
        print(f"\n ALL TESTS PASSED!")
        print(f"  Total execution time: {total_time:.3f} seconds")
        print("=" * 60)
        
    except Exception as e:
        total_time = time.time() - start_time
        print(f"\n[FAILED] TEST FAILED!")
        print(f"[ERROR] Error: {str(e)}")
        print(f"[TIME] Execution time before failure: {total_time:.3f} seconds")
        print("=" * 60)
        raise
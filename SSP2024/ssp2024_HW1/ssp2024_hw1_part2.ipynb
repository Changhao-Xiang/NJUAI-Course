{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sd_NB3PHRMpB"
   },
   "source": [
    "# PART 2\n",
    "This part includes 7 problems that sums up to 60 points. This part aims to help you deepen your knowledge related to audio , which includes audio IO, quantization, sampling and short-time analysis. 60 points totally in this part."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3SXSjGUbUXkH"
   },
   "source": [
    "# Audio IO\n",
    "\n",
    "First, you need to import the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "RhO9YEWZSWop"
   },
   "outputs": [],
   "source": [
    "# Load required libraries\n",
    "import librosa  # for some audio processing\n",
    "import librosa.display\n",
    "import IPython.display as ipd  # for playing audios\n",
    "import matplotlib.pyplot as plt  # matplot lib is the premiere plotting lib for Python: https://matplotlib.org/\n",
    "import numpy as np  # numpy is the premiere signal handling library for Python: http://www.numpy.org/\n",
    "import scipy as sp  # for signal processing\n",
    "from scipy import signal\n",
    "import random\n",
    "from utils import *\n",
    "import struct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Problem 1** (10 points): Reading Wave file\n",
    "You should write a function to load and read important fields from a Wave file. Note that you are not allowed to use any existing functions from librosa and similar libraries. Instead, you need to read file in binary and parse the audio header to get such information. Please refer to http://soundfile.sapp.org/doc/WaveFormat/ for more information about Wave format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IV-2nstxVJNa"
   },
   "outputs": [],
   "source": [
    "audio_path = \"data/music.wav\"\n",
    "# audio_path = \"data/HumanVoice-Hello_16bit_44.1kHz_mono.wav\"\n",
    "\n",
    "## Important Note: the above audio file contains a LIST chunk after the fmt subchunk,\n",
    "## which is different from described in http://soundfile.sapp.org/doc/WaveFormat/,\n",
    "## You should skip the LIST chunk to reach the data chunk\n",
    "\n",
    "\n",
    "def read_wavefile(audio_path):\n",
    "    \"\"\"Read a wavefile and return:\n",
    "\n",
    "    sample_rate: the sample rate of the wave form\n",
    "    num_channels: the number of channes (1 for mono, 2 for stereo)\n",
    "    number_of_samples: the number of audio samples\n",
    "    quantization: 1 for uniform quantization and 2 for mu-quantization\n",
    "    data: a numpy array of size [num_channels, number_of_samples]\n",
    "    duration: the length of the audio file in second\n",
    "    \"\"\"\n",
    "    ## YOUR CODE HERE\n",
    "    with open(audio_path, \"rb\") as f:\n",
    "        # Read the RIFF header\n",
    "        riff_header = f.read(4)\n",
    "        if riff_header != b\"RIFF\":\n",
    "            raise ValueError(\"Not a valid WAV file\")\n",
    "\n",
    "        file_size = struct.unpack(\"<I\", f.read(4))[0]\n",
    "\n",
    "        wave_id = f.read(4)\n",
    "        if wave_id != b\"WAVE\":\n",
    "            raise ValueError(\"Not a valid WAV file\")\n",
    "\n",
    "        fmt_header = f.read(4)\n",
    "        if fmt_header != b\"fmt \":\n",
    "            raise ValueError(\"fmt subchunk not found\")\n",
    "\n",
    "        fmt_chunk_size = struct.unpack(\"<I\", f.read(4))[0]\n",
    "\n",
    "        audio_format = struct.unpack(\"<H\", f.read(2))[0]\n",
    "        quantization = 1 if audio_format == 1 else 2\n",
    "\n",
    "        num_channels = struct.unpack(\"<H\", f.read(2))[0]\n",
    "\n",
    "        sample_rate = struct.unpack(\"<I\", f.read(4))[0]\n",
    "\n",
    "        byte_rate = struct.unpack(\"<I\", f.read(4))[0]\n",
    "        block_align = struct.unpack(\"<H\", f.read(2))[0]\n",
    "\n",
    "        bits_per_sample = struct.unpack(\"<H\", f.read(2))[0]\n",
    "\n",
    "        # Skip any extra bytes in the fmt chunk\n",
    "        if fmt_chunk_size > 16:\n",
    "            f.read(fmt_chunk_size - 16)\n",
    "\n",
    "        # Skip the LIST chunk if present\n",
    "        while True:\n",
    "            chunk_header = f.read(4)\n",
    "            if chunk_header == b\"LIST\":\n",
    "                list_chunk_size = struct.unpack(\"<I\", f.read(4))[0]\n",
    "                f.read(list_chunk_size)\n",
    "            else:\n",
    "                f.seek(-4, 1)\n",
    "                break\n",
    "\n",
    "        data_header = f.read(4)\n",
    "        if data_header != b\"data\":\n",
    "            raise ValueError(\"data subchunk not found\")\n",
    "\n",
    "        data_chunk_size = struct.unpack(\"<I\", f.read(4))[0]\n",
    "\n",
    "        data = np.frombuffer(f.read(data_chunk_size), dtype=np.int16)\n",
    "\n",
    "        # Reshape the data to [num_channels, number_of_samples]\n",
    "        number_of_samples = data_chunk_size // (num_channels * (bits_per_sample // 8))\n",
    "        data = data.reshape((num_channels, number_of_samples))\n",
    "\n",
    "        duration = number_of_samples / sample_rate\n",
    "\n",
    "        # Normalize data with int16\n",
    "        data = data / (2**15)\n",
    "\n",
    "    return sample_rate, num_channels, number_of_samples, quantization, data, duration\n",
    "\n",
    "\n",
    "sr, n_channels, n_samples, quantization, data, duration = read_wavefile(audio_path)\n",
    "print(\"sample_rate:\", sr)\n",
    "print(\"num_channels:\", n_channels)\n",
    "print(\"number_of_samples:\", n_samples)\n",
    "print(\"quantization:(PCM)\", quantization)\n",
    "print(\"data:\", data)\n",
    "print(\"duration:\", duration)\n",
    "\n",
    "# Verify that your answer is correct by loading the file using librosa library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "be277021"
   },
   "source": [
    "## Verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gTBoDrU3VMtJ"
   },
   "outputs": [],
   "source": [
    "audio_path = \"data/music.wav\"\n",
    "audio_data, sample_rate = librosa.load(audio_path, sr=None, mono=False)\n",
    "print(sample_rate)\n",
    "print(audio_data)\n",
    "print(np.shape(audio_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Otx-MEmDVM0g"
   },
   "outputs": [],
   "source": [
    "audio_path = \"data/HumanVoice-Hello_16bit_44.1kHz_mono.wav\"\n",
    "audio_data, sample_rate = librosa.load(audio_path, sr=None, mono=False)\n",
    "print(sample_rate)\n",
    "print(audio_data)\n",
    "print(np.shape(audio_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2ae19cc8"
   },
   "source": [
    "# Quantization\n",
    "\n",
    "Quantization convert values of an audio sequence from continuous, real values to discreate values from a set of `2^quantization_bits`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "41678274",
    "outputId": "70f7c576-97c6-44a1-c7e0-5158b41b70e9"
   },
   "outputs": [],
   "source": [
    "from utils import plot_signal\n",
    "\n",
    "# Load a sample file with 16 bits quantization\n",
    "sampling_rate, audio_data_16bit = sp.io.wavfile.read(\"data/HumanVoice-Hello_16bit_44.1kHz_mono.wav\")\n",
    "\n",
    "print(f\"Sampling rate: {sampling_rate} Hz\")\n",
    "print(f\"Number of channels = {len(audio_data_16bit.shape)}\")\n",
    "print(f\"Total samples: {audio_data_16bit.shape[0]}\")\n",
    "\n",
    "\n",
    "length_in_secs = audio_data_16bit.shape[0] / sampling_rate\n",
    "quantization_bits = 16\n",
    "print(\n",
    "    f\"{quantization_bits}-bit audio ranges from -{2**(quantization_bits - 1)} to {2**(quantization_bits - 1) - 1}\"\n",
    ")\n",
    "print(f\"Max value: {np.max(audio_data_16bit)} Avg value: {np.mean(audio_data_16bit):.2f}\")\n",
    "\n",
    "# We'll highlight and zoom in on the orange part of the graph controlled by xlim_zoom\n",
    "xlim_zoom = (11000, 12500)  # you may want to change this depending on what audio file you have loaded\n",
    "plot_signal(audio_data_16bit, sampling_rate, quantization_bits, xlim_zoom=xlim_zoom)\n",
    "ipd.Audio(audio_data_16bit, rate=sampling_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c59378d0"
   },
   "source": [
    "### **Problem 2** (10 points): Experiment with Quantization with different number of bits\n",
    "\n",
    "Since we do not have the original signal, we pretend that the previously loaded audio is the original one and try to quantize the signal with a changing number of quantization bits. Your task is as follows:\n",
    "\n",
    "- Experiment with 8-bit, 6-bit, 4-bit, 3-bit and 2-bit quantization. For each quantization level, play the quantized audio data and display the signal as demonstrated with 16-bit quantization above.\n",
    "- Write a short discussion on the quality of resulting signals with different levels of quantization.\n",
    "- Implement mu-quantization with 6-bit quantization level. Play the resulting signal and compare the quality with the signal with the same level of bit (6-bit) in uniform quantization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6527b63d"
   },
   "source": [
    "#### Uniform Quantization 8-bit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q7fMHxJ7Xdvg"
   },
   "outputs": [],
   "source": [
    "audio_original_data = (audio_data_16bit * 2**16).astype(\"int64\")\n",
    "# why *(2**16) here?\n",
    "# or why not /(2**16) for normalization?\n",
    "\n",
    "\n",
    "def uniform_quantize(audio_original_data, quantization_bits):\n",
    "    \"\"\"\n",
    "    Return the quantized data\n",
    "    \"\"\"\n",
    "    ## YOUR CODE HERE\n",
    "    # Calculate the quantization step size\n",
    "    num_levels = 2**quantization_bits\n",
    "    # max_original = np.max(audio_original_data)\n",
    "    # min_original = np.min(audio_original_data)\n",
    "    max_original = 2**31\n",
    "    min_original = -(2**31)\n",
    "    step_size = (max_original - min_original) / num_levels\n",
    "\n",
    "    # Quantize the audio data\n",
    "    quantized_data = np.floor((audio_original_data - min_original) / step_size + 0.5)\n",
    "    quantized_data = np.clip(quantized_data, 0, num_levels - 1) - (num_levels // 2)\n",
    "\n",
    "    # Convert back to the original data type\n",
    "    quantized_data = quantized_data.astype(audio_original_data.dtype)\n",
    "\n",
    "    return quantized_data\n",
    "\n",
    "\n",
    "quantization_bits = 8\n",
    "audio_data_8bit = uniform_quantize(audio_original_data, quantization_bits)\n",
    "print(\n",
    "    f\"{quantization_bits}-bit audio ranges from -{2**(quantization_bits - 1)} to {2**(quantization_bits - 1) - 1}\"\n",
    ")\n",
    "print(f\"Max value: {np.max(audio_data_8bit)} Avg value: {np.mean(audio_data_8bit):.2f}\")\n",
    "\n",
    "# We'll highlight and zoom in on the orange part of the graph controlled by xlim_zoom\n",
    "xlim_zoom = (11000, 12500)  # you may want to change this depending on what audio file you have loaded\n",
    "plot_signal(audio_data_8bit, sampling_rate, quantization_bits, xlim_zoom=xlim_zoom)\n",
    "ipd.Audio(audio_data_8bit, rate=sampling_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "33339ff6"
   },
   "source": [
    "#### Uniform Quantization 6-bit\n",
    "\n",
    "Perform the similar procedure as the one with `quantization_bits = 6`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FI9_OarQXi_q"
   },
   "outputs": [],
   "source": [
    "quantization_bits = 6\n",
    "audio_data_6bit = uniform_quantize(audio_original_data, quantization_bits)\n",
    "print(\n",
    "    f\"{quantization_bits}-bit audio ranges from -{2**(quantization_bits - 1)} to {2**(quantization_bits - 1) - 1}\"\n",
    ")\n",
    "print(f\"Max value: {np.max(audio_data_6bit)} Avg value: {np.mean(audio_data_6bit):.2f}\")\n",
    "\n",
    "# We'll highlight and zoom in on the orange part of the graph controlled by xlim_zoom\n",
    "xlim_zoom = (11000, 12500)  # you may want to change this depending on what audio file you have loaded\n",
    "plot_signal(audio_data_6bit, sampling_rate, quantization_bits, xlim_zoom=xlim_zoom)\n",
    "ipd.Audio(audio_data_6bit, rate=sampling_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "681c579a"
   },
   "source": [
    "#### Uniform Quantization 4-bit\n",
    "\n",
    "Perform the similar procedure as above with `quantization_bits = 4`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vpQDnedAXmOq"
   },
   "outputs": [],
   "source": [
    "quantization_bits = 4\n",
    "audio_data_4bit = uniform_quantize(audio_original_data, quantization_bits)\n",
    "print(\n",
    "    f\"{quantization_bits}-bit audio ranges from -{2**(quantization_bits - 1)} to {2**(quantization_bits - 1) - 1}\"\n",
    ")\n",
    "print(f\"Max value: {np.max(audio_data_4bit)} Avg value: {np.mean(audio_data_4bit):.2f}\")\n",
    "\n",
    "# We'll highlight and zoom in on the orange part of the graph controlled by xlim_zoom\n",
    "xlim_zoom = (11000, 12500)  # you may want to change this depending on what audio file you have loaded\n",
    "plot_signal(audio_data_4bit, sampling_rate, quantization_bits, xlim_zoom=xlim_zoom)\n",
    "ipd.Audio(audio_data_4bit, rate=sampling_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5df2198b"
   },
   "source": [
    "#### Uniform Quantization 3-bit\n",
    "\n",
    "Perform the similar procedure as above with `quantization_bits = 3`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8FXO0RJhXoqh"
   },
   "outputs": [],
   "source": [
    "quantization_bits = 3\n",
    "audio_data_3bit = uniform_quantize(audio_original_data, quantization_bits)\n",
    "print(\n",
    "    f\"{quantization_bits}-bit audio ranges from -{2**(quantization_bits - 1)} to {2**(quantization_bits - 1) - 1}\"\n",
    ")\n",
    "print(f\"Max value: {np.max(audio_data_3bit)} Avg value: {np.mean(audio_data_3bit):.2f}\")\n",
    "\n",
    "# We'll highlight and zoom in on the orange part of the graph controlled by xlim_zoom\n",
    "xlim_zoom = (11000, 12500)  # you may want to change this depending on what audio file you have loaded\n",
    "plot_signal(audio_data_3bit, sampling_rate, quantization_bits, xlim_zoom=xlim_zoom)\n",
    "ipd.Audio(audio_data_3bit, rate=sampling_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d17ad706"
   },
   "source": [
    "#### Uniform Quantization 2-bit\n",
    "\n",
    "Perform the similar procedure as above with `quantization_bits = 2`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PNkSi7HPXrGw"
   },
   "outputs": [],
   "source": [
    "quantization_bits = 2\n",
    "audio_data_2bit = uniform_quantize(audio_original_data, quantization_bits)\n",
    "print(\n",
    "    f\"{quantization_bits}-bit audio ranges from -{2**(quantization_bits - 1)} to {2**(quantization_bits - 1) - 1}\"\n",
    ")\n",
    "print(f\"Max value: {np.max(audio_data_2bit)} Avg value: {np.mean(audio_data_2bit):.2f}\")\n",
    "\n",
    "# We'll highlight and zoom in on the orange part of the graph controlled by xlim_zoom\n",
    "xlim_zoom = (11000, 12500)  # you may want to change this depending on what audio file you have loaded\n",
    "plot_signal(audio_data_2bit, sampling_rate, quantization_bits, xlim_zoom=xlim_zoom)\n",
    "ipd.Audio(audio_data_2bit, rate=sampling_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e093249c"
   },
   "source": [
    "#### **Discussion**\n",
    "\n",
    "The quality of the resulting signals decreases as the levels of quantization decrease, especially when the number of bits is less than 6."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4eab4587"
   },
   "source": [
    "### **Problem 3** (10 points): Mu-Quantization\n",
    "\n",
    "Implement mu-quantization with 6-bit quantization level. Play the resulting signal and compare the quality with the signal with the same level of bit (6-bit) in uniform quantization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VeYhDYNWYtwS"
   },
   "outputs": [],
   "source": [
    "def normalize(x):\n",
    "    \"\"\"\n",
    "    Convert a signal from range [min, max] to [-1,1]\n",
    "    \"\"\"\n",
    "    max_x = np.max(x)\n",
    "    min_x = np.min(x)\n",
    "    normalized_x = 2 * (x - min_x) / (max_x - min_x) - 1\n",
    "    return normalized_x\n",
    "\n",
    "\n",
    "def mu_compress(x, mu=255):\n",
    "    ### YOUR CODE FOR MU-compression\n",
    "    x_comp = np.sign(x) * np.log(1 + mu * np.abs(x)) / np.log(1 + mu)\n",
    "    return x_comp\n",
    "\n",
    "\n",
    "def mu_expand(x_comp, mu=255):\n",
    "    \"\"\"\n",
    "    Convert the compressed signal to the original signal\n",
    "    \"\"\"\n",
    "    ### YOUR CODE FOR MU-expansion\n",
    "    x = np.sign(x_comp) * ((1 + mu) ** np.abs(x_comp) - 1) / mu\n",
    "    return x\n",
    "\n",
    "\n",
    "normalized_audio_data = normalize(audio_original_data)\n",
    "compressed_audio_data = mu_compress(normalized_audio_data)\n",
    "quantization_bits = 6\n",
    "\n",
    "max_x = np.max(compressed_audio_data)\n",
    "min_x = np.min(compressed_audio_data)\n",
    "step_size = (max_x - min_x) / np.power(2, quantization_bits)\n",
    "\n",
    "\n",
    "### YOUR CODE HERE\n",
    "\n",
    "# perform uniform quantization\n",
    "quantized_data = np.floor((compressed_audio_data - min_x) / step_size + 0.5) * step_size + min_x\n",
    "# restore the ``unquantized signal'' (you may need to keep the min, max of the signal before uniform quantization)\n",
    "unquantized_signal = ((quantized_data - min_x) / step_size - 0.5) * step_size + min_x + 0.0625\n",
    "# use mu_expand to restore the signal before compression\n",
    "restored_signal = mu_expand(unquantized_signal)\n",
    "# play the audio\n",
    "ipd.Audio(restored_signal, rate=sampling_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6132fe73"
   },
   "source": [
    "#### **Discussion**\n",
    "The mu-quantized signal has weaker noise / higher SNR compared to the signal with the same level of 6-bits uniform quantization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "955b95cb"
   },
   "source": [
    "# Sampling\n",
    "\n",
    "### **Problem 4** (5 points): Experiments with Nyquist limit\n",
    "\n",
    "In this part, you will experiment with Nyquist limit. The requirement for this part is as follows:\n",
    "- Write code to generate samples of a sweeping signal, which is the signal with frequency changing over time. The frequency range is from 0 - 22,050Hz within 30 seconds, with the sampling rate of 44,100 Hz. Play the audio and notice how you can't hear any sound when the signal is played from a particular timestamp. Explain the phenomenon.\n",
    "- Write code to change the sampling rate to 11,025 Hz, 882 Hz, play audio and display the corresponding spectrum. Write your discussion on the observations. Note that for the sampling rate of 882Hz, you may want to use Firefox for running your notebooks as Chrome has some issues when playing signals with a low sampling rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Nu0V1KCzaBo7"
   },
   "outputs": [],
   "source": [
    "## Generate samples of a sweeping signal with sampling rate 44,100\n",
    "## Perform uniform quantization with quantization_bits of 16\n",
    "quantization_bits = 16\n",
    "sampling_rate = 44100\n",
    "\n",
    "## TODO: YOUR CODE HERE\n",
    "duration = 30\n",
    "start_freq = 0  # Starting frequency in Hz\n",
    "end_freq = 22050  # Ending frequency in Hz\n",
    "\n",
    "# get linear_chirp\n",
    "t = np.linspace(0, duration, duration * sampling_rate)\n",
    "linear_chirp = signal.chirp(t, start_freq, duration, end_freq, method=\"linear\")\n",
    "\n",
    "# Set a zoom area (a bit hard to see but highlighted in red in spectrogram)\n",
    "xlim_zoom = (11500, 15500)\n",
    "plot_signal_and_spectrogram(linear_chirp, sampling_rate, quantization_bits, xlim_zoom=xlim_zoom)\n",
    "ipd.Audio(linear_chirp, rate=sampling_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Usz6fkk-aLV1"
   },
   "outputs": [],
   "source": [
    "## Generate samples of a sweeping signal with sampling rate 11,025\n",
    "quantization_bits = 16\n",
    "sampling_rate = 11025\n",
    "\n",
    "## TODO: YOUR CODE HERE\n",
    "duration = 30\n",
    "start_freq = 0  # Starting frequency in Hz\n",
    "end_freq = 22025  # Ending frequency in Hz\n",
    "\n",
    "# get linear_chirp\n",
    "t = np.linspace(0, duration, duration * sampling_rate)\n",
    "linear_chirp = signal.chirp(t, start_freq, duration, end_freq, method=\"linear\")\n",
    "\n",
    "# Set a zoom area (a bit hard to see but highlighted in red in spectrogram)\n",
    "xlim_zoom = (11500, 15500)\n",
    "plot_signal_and_spectrogram(linear_chirp, sampling_rate, quantization_bits, xlim_zoom=xlim_zoom)\n",
    "ipd.Audio(linear_chirp, rate=sampling_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lkbHy90waPtO"
   },
   "outputs": [],
   "source": [
    "## Generate samples of a sweeping signal with sampling rate 882\n",
    "quantization_bits = 16\n",
    "sampling_rate = 882\n",
    "\n",
    "## TODO: YOUR CODE HERE\n",
    "duration = 30\n",
    "start_freq = 0  # Starting frequency in Hz\n",
    "end_freq = 22025  # Ending frequency in Hz\n",
    "\n",
    "# get linear_chirp\n",
    "t = np.linspace(0, duration, duration * sampling_rate)\n",
    "linear_chirp = signal.chirp(t, start_freq, duration, end_freq, method=\"linear\")\n",
    "\n",
    "# Set a zoom area (a bit hard to see but highlighted in red in spectrogram)\n",
    "xlim_zoom = (11500, 15500)\n",
    "plot_signal_and_spectrogram(linear_chirp, sampling_rate, quantization_bits, xlim_zoom=xlim_zoom)\n",
    "ipd.Audio(linear_chirp, rate=sampling_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "206f4ca2-af47-4010-a286-8efc080ff59a"
   },
   "source": [
    "### **Discussion**\n",
    "\n",
    "- I can't hear any sound when the signal is played for about 20s because human ears can perceive frequencies up to about 20,000 Hz.\n",
    "- When changing the sampling rate to 11025 Hz or 882 Hz, high-frequency components (with _f_ > 5512.5 Hz or _f_ > 441 Hz) of the original signal is missed due to alias."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "59473191-25f3-4251-ad8b-7d7552189eb0"
   },
   "source": [
    "# Frequency Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "31993f2e-c6a8-43e0-a08a-d7102fa2a747"
   },
   "source": [
    "### **Prolem 5** (5 points):  main frequency components\n",
    "In this problem, you are given the file `samples.txt`, which contain  samples obtained with the sampling rate of 800 (samples/second) from an original signal with unknown frequency. Use the **autocorrelation method** to detect the frequency of the original signal and write down the explaination for your answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ei9r-PIEbKaQ"
   },
   "outputs": [],
   "source": [
    "with open(\"data/samples.txt\", \"r\") as file:\n",
    "    samples = [float(line.strip()) for line in file]\n",
    "\n",
    "### TODO: YOUR CODE HERE\n",
    "\n",
    "# Calculate the autocorrelation of the samples\n",
    "autocorr = np.correlate(samples, samples, mode=\"full\")\n",
    "autocorr = autocorr[len(autocorr) // 2 :]  # Keep only the positive lags\n",
    "\n",
    "# Find the first peak in the autocorrelation function\n",
    "first_peak_index = np.argmax(autocorr[1:]) + 1\n",
    "\n",
    "# Calculate the frequency based on the first peak\n",
    "sampling_rate = 800\n",
    "frequency = sampling_rate / first_peak_index\n",
    "\n",
    "print(\"Detected Frequency: {:.2f} Hz\".format(frequency))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "32dbaa64"
   },
   "source": [
    "**Note**: If there is a significant periodic component in the signal, the autocorrelation calculation will show these periodic features. When the signal repeats within a period, the autocorrelation function will show significant peaks. The spacing of these peak positions can determine the period of the data and thus the main frequency components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eea7c2bd-bef8-40ec-a86f-ab35f9d24e7a"
   },
   "source": [
    "### **Problem 6** (10 points): STFT Analysis\n",
    "\n",
    "Analyze the spectrogram with STFT with different sizes of window length, and observe on the relationship of the window size and the resulting spectrograms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jjVEFqtlb0EL"
   },
   "outputs": [],
   "source": [
    "y, sr = librosa.load(librosa.ex(\"trumpet\"))\n",
    "\n",
    "## TODO:\n",
    "### 1. Perform STFT analysis with win_length of 125 and hop_length of 64.\n",
    "### 2. Calculate the power spectrum\n",
    "### 3. Display the spectrogram\n",
    "### Note: You can use Librosa for this part\n",
    "\n",
    "## YOUR CODE HERE\n",
    "stft_125 = librosa.stft(y, n_fft=2048, hop_length=64, win_length=125)\n",
    "db_125 = librosa.amplitude_to_db(np.abs(stft_125), ref=np.max)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "img = librosa.display.specshow(db_125, y_axis=\"mel\", x_axis=\"time\", ax=ax)\n",
    "ax.set_title(\"Power spectrogram\")\n",
    "fig.colorbar(img, ax=ax, format=\"%+2.0f dB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iiynSf1-b4tE"
   },
   "outputs": [],
   "source": [
    "## TODO: Do the same as the above but with the win_length of 256\n",
    "## YOUR CODE HERE\n",
    "stft_256 = librosa.stft(y, n_fft=2048, hop_length=64, win_length=256)\n",
    "db_256 = librosa.amplitude_to_db(np.abs(stft_256), ref=np.max)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "img = librosa.display.specshow(db_256, y_axis=\"mel\", x_axis=\"time\", ax=ax)\n",
    "ax.set_title(\"Power spectrogram\")\n",
    "fig.colorbar(img, ax=ax, format=\"%+2.0f dB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YzsLFM-PcHWb"
   },
   "outputs": [],
   "source": [
    "## TODO: Do the same as the above but with the win_length of 512\n",
    "## YOUR CODE HERE\n",
    "stft_512 = librosa.stft(y, n_fft=2048, hop_length=64, win_length=512)\n",
    "db_512 = librosa.amplitude_to_db(np.abs(stft_512), ref=np.max)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "img = librosa.display.specshow(db_512, y_axis=\"mel\", x_axis=\"time\", ax=ax)\n",
    "ax.set_title(\"Power spectrogram\")\n",
    "fig.colorbar(img, ax=ax, format=\"%+2.0f dB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "676beecc-fb42-49aa-be42-01a6c235bc41"
   },
   "source": [
    "#### **Discussion**\n",
    "On the one hand, as window length of STFT become longer, the frequency resolution becomes higher. We can see different frequency components more clearly with longer window.\n",
    "\n",
    "On the other hand, longer window brings lower time resolution, leading to smoothed-out transients and a delayed response to changes in the signal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b98bd10a-0df3-4fd2-9484-e9a2da8aa3ae"
   },
   "source": [
    "### **Problem 7** (10 points): program our own stft function. \n",
    "\n",
    "You cannot use the one in the `librosa` package. To compute fast fourier transforms, use `scipy.fft.fft`.\n",
    "\n",
    "Concretely, given an audio signal of\n",
    "```\n",
    "[ 1 2 3 4 5 6 7 8 9 10 ]\n",
    "```\n",
    "a `hop_length` of 2 and a `win_length` of 3 would result in the following windowed segments\n",
    "```\n",
    "[ 1 2 3 ] [ 3 4 5 ] [ 5 6 7 ] [ 7 8 9]\n",
    "```\n",
    "(here `n_windows = 3`).  \n",
    "\"\"\"why not `n_windows = 4`?\"\"\"\n",
    "\n",
    "Each of these windows would then be put through a fast fourier transform and concatenated along the first dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "e6fcb70f-3d3a-40c1-be5f-e481d2d5d475"
   },
   "outputs": [],
   "source": [
    "from scipy.fft import fft\n",
    "\n",
    "\n",
    "def simple_stft(y, sr, hop_length=512, win_length=2048, n_fft=2048):\n",
    "    \"\"\"\n",
    "    A simple short-time fourier transform.\n",
    "\n",
    "    Arguments\n",
    "    ---------\n",
    "    y: numpy array\n",
    "       audio signal\n",
    "    hop_length : integer [default: 512]\n",
    "                 the number of samples to skip in between windows.\n",
    "    win_length : integer [default: 2048]\n",
    "                 the size of a single windowed segment.\n",
    "    n_fft: integer [default: 2048]\n",
    "                 number of frequencies for fast fourier transforms.\n",
    "\n",
    "    Outputs\n",
    "    -------\n",
    "    stft_matrix : numpy array\n",
    "            shape: n_windows x n_fft\n",
    "            contains amplitudes for the spectrogram.\n",
    "    \"\"\"\n",
    "    # Calculate the number of windows\n",
    "    n_windows = (len(y) - win_length) // hop_length + 1\n",
    "\n",
    "    # Initialize the STFT matrix\n",
    "    stft_matrix = np.zeros((n_windows, n_fft), dtype=np.complex_)\n",
    "\n",
    "    # Create the Hann window\n",
    "    window = np.hanning(win_length)\n",
    "\n",
    "    for i in range(n_windows):\n",
    "        # Extract the windowed segment\n",
    "        start = i * hop_length\n",
    "        end = start + win_length\n",
    "        segment = y[start:end] * window\n",
    "\n",
    "        # Compute the FFT and store the result\n",
    "        stft_matrix[i, :n_fft] = fft(segment, n=n_fft)\n",
    "\n",
    "    return stft_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tUiBeJKEdXOV"
   },
   "outputs": [],
   "source": [
    "### get the spectrogram\n",
    "from scipy.fft import rfft, fft\n",
    "\n",
    "y, sr = librosa.load(librosa.ex(\"trumpet\"))\n",
    "\n",
    "S = np.abs(simple_stft(y, sr))\n",
    "fig, ax = plt.subplots()\n",
    "img = librosa.display.specshow(librosa.amplitude_to_db(S, ref=np.max))\n",
    "ax.set_title(\"Power spectrogram\")\n",
    "fig.colorbar(img, ax=ax, format=\"%+2.0f dB\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPNYmAhFiC5Pz7p8N/k+Ur0",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

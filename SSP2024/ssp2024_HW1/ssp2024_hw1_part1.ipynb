{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4R-6N5YMWPkg"
   },
   "source": [
    "# SSP2024: Homework 1 Notebook.\n",
    "\n",
    "# Overview\n",
    "\n",
    "In this homework we will use [Librosa](https://librosa.org/doc/latest/index.html) and [pysptk](http://sp-tk.sourceforge.net/). Both of these libraries have implementations of speech analysis and signal processing algorithms along with helper functions for data loading and visualization. You can find more documentation about each of these online ([Librosa docs](https://librosa.org/doc/latest/index.html), [pysptk github](https://github.com/r9y9/pysptk)).\n",
    "\n",
    "\n",
    "**Deadline: 2024.11.12 24:00**\n",
    "\n",
    "You have 3 days late policy. Any submissions within three days of the deadline can still be accepted but with penalty. Submission after three days of the deadline will not be accepted and marked as zero. This homework have two parts totally."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "msNfHxkq8jEs"
   },
   "source": [
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "# PART 1\n",
    "\n",
    "This part will introduce you to the processing procedures for the audio data. There are a total of 10 problems in this part. The tasks in this part make up 50 points of the overall 110 for Homework 1. First, you need to install and import the necessary libraries. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xsNSLPyVRIfn"
   },
   "outputs": [],
   "source": [
    "!pip install \"librosa~=0.10.1\" \"matplotlib~=3.7.1\" pysptk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "eu8O0ZnO8jEu"
   },
   "outputs": [],
   "source": [
    "import pysptk\n",
    "from pysptk.synthesis import MLSADF, Synthesizer\n",
    "import librosa\n",
    "import librosa.display\n",
    "import librosa.effects\n",
    "import librosa.util\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# used to play audio files\n",
    "import IPython.display as ipd\n",
    "from base64 import b64decode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "On2AkQ7w8Owo"
   },
   "source": [
    "## I. Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "igLsfDkN8jEw"
   },
   "source": [
    " In this homework, we will take our first glance at a handful of raw audio clips between bank employees and customers from the [HarperValleyBank dataset](https://arxiv.org/abs/2010.13929). Working with audio files in this way is similar to what you might experience when working with data exported from call center recordings or similar telephone/app-based human-human interactions. To start, ensure you can execute the commands below to locate and load the audio files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ip-0rxFzRMuM"
   },
   "outputs": [],
   "source": [
    "# download dataset from public GDrive\n",
    "!wget -O hvb_sampledata.zip https://drive.usercontent.google.com/download?id=1tFaTu7LWuCxn3DkAyudGGyv6eilVO5pC\n",
    "# https://drive.google.com/file/d/1tFaTu7LWuCxn3DkAyudGGyv6eilVO5pC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 898,
     "status": "ok",
     "timestamp": 1729130077011,
     "user": {
      "displayName": "Jyan L",
      "userId": "02978963593103718267"
     },
     "user_tz": -480
    },
    "id": "Au1QBJTS9xY9",
    "outputId": "9e86c5dd-df93-4a26-d38b-fa69656d029c"
   },
   "outputs": [],
   "source": [
    "!ls\n",
    "!unzip -o hvb_sampledata.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oL-kHJ8oRQry"
   },
   "outputs": [],
   "source": [
    "ls sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rO1BcOwz8jEw"
   },
   "source": [
    "In the `sample` folder, we've put a few example conversations between an agent (bank employee) and a caller (bank customer). Each conversation is split into two `.wav` files by speaker. For example, `sample/agent/0002f70f7386445b.wav` and `sample/caller/0002f70f7386445b.wav` belong to the same conversation. The first file contains everything the agent says while the second contains everything the caller says. This speaker-separated format is common in speech corpora."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6YHEiQlgRUDK"
   },
   "outputs": [],
   "source": [
    "ls sample/agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GFvF0r4CRUGw"
   },
   "outputs": [],
   "source": [
    "ls sample/caller"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5ZutsTX4B3zc"
   },
   "source": [
    "## II. Playing audio files & Loading into an array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9YKbn95J8jEx"
   },
   "source": [
    "We can listen to the audio files to familiarize ourselves with the data. The long pauses you hear between utterances is because each audio file is from only one side of the conversation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 75
    },
    "executionInfo": {
     "elapsed": 1003,
     "status": "ok",
     "timestamp": 1729130097231,
     "user": {
      "displayName": "Jyan L",
      "userId": "02978963593103718267"
     },
     "user_tz": -480
    },
    "id": "qhWL9mrs8jEy",
    "outputId": "e7e0848e-440a-4dc6-a879-bb52d16f3f55"
   },
   "outputs": [],
   "source": [
    "# agent side of the conversation\n",
    "ipd.Audio(\"./sample/agent/0002f70f7386445b.wav\", rate=22050)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 75
    },
    "executionInfo": {
     "elapsed": 1280,
     "status": "ok",
     "timestamp": 1729130100488,
     "user": {
      "displayName": "Jyan L",
      "userId": "02978963593103718267"
     },
     "user_tz": -480
    },
    "id": "NpbJaNkY8jEy",
    "outputId": "329cd9d1-3fdc-43ce-8cc9-56b46260fabd"
   },
   "outputs": [],
   "source": [
    "# caller side of the conversation\n",
    "ipd.Audio(\"./sample/caller/0002f70f7386445b.wav\", rate=22050)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FS9lKSnh8jEz"
   },
   "source": [
    "When working with spoken language, we often want to visualize and process the raw audio before applying machine learning or signal processing approaches. To do this, we typically work with an audio file as an array or tensor. Our audio files are a single channel (mono, not stereo), or we sometimes have a two-party conversation encoded as stereo with one speaker in the left/right channel for separation.\n",
    "\n",
    "The `librosa` package contains a suite of utilities to open and process waveforms. The function `librosa.load` loads an audio file as a floating point time series. It will return a numpy array containing samples from the audio clip and the sample rate (`sr`) listed in the audio file header. The sample rate is an integeter for how many samples (array indices) correspond to 1 second of realtime audio. The higher the sample rate, the higher the \"resolution\" of an audio clip."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "AuIBxfTo8jEz"
   },
   "outputs": [],
   "source": [
    "# sr represents a \"sample rate\"\n",
    "wav_agent, sr_agent = librosa.load(\"./sample/agent/0002f70f7386445b.wav\")\n",
    "wav_caller, sr_caller = librosa.load(\"./sample/caller/0002f70f7386445b.wav\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6zOJb8eQ8jEz"
   },
   "source": [
    "Like above, there are long pauses in each clip where the other speaker is speaking.  These \"silence\" gaps are intended to allow users to recover the original conversation if needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bPwpperJFeAv"
   },
   "source": [
    "### **Problem 1 (8 points): Combine both conversation sides into a single array** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zXGjtR0w8jEz"
   },
   "source": [
    "To familiarize yourself with waveforms and `librosa`, merge the agent and caller waveforms into a single waveform. You may assume that the audio files to combine will always have the same sampling rate `(sr_caller == sr_agent)`. If two files are unequal length, your combined audio should be length = max( length_1, length_2), using 0 padding when necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "4tRSu7Gx8jE1"
   },
   "outputs": [],
   "source": [
    "merged_wav = None  # TODO: define this variable\n",
    "sr_merged = sr_agent\n",
    "\n",
    "#############################\n",
    "#### YOUR CODE GOES HERE ####\n",
    "max_length = max(len(wav_agent), len(wav_caller))\n",
    "merged_wav = np.zeros(max_length)\n",
    "merged_wav[: len(wav_agent)] = wav_agent\n",
    "merged_wav[: len(wav_caller)] = np.maximum(merged_wav[: len(wav_caller)], wav_caller)\n",
    "\n",
    "#############################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rtOH4be9F46D"
   },
   "source": [
    "Use `ipd.Audio` to play the merged waveform. It should sound like a 2-person conversation now!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1qI-iiPLGXgC"
   },
   "outputs": [],
   "source": [
    "ipd.Audio(merged_wav, rate=sr_merged)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bR3K1cvGN8jr"
   },
   "source": [
    "## III. Visualizing audio using time-frequency spectrograms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UYOnYfIt8jE2"
   },
   "source": [
    "In general, it is hard to directly work with waveforms in machine learning. There are differences in magnitude, variable lengths, and speech-relevant patterns are hard to discern by visualizing a waveform directly. We often rely on signal processing tools to \"standardize\" waveforms. In speech, it is common to convert raw waveforms to the frequency domain with _spectrograms_. A spectrogram is a time series of short-window fourier transforms, so we can see how the frequencies of speech change over the course of an utterance.\n",
    "\n",
    "The human auditory system does not perceive all frequencies equally. In very low or high frequencies (in hertz), our ears are less capable at discriminating between different frequencies.\n",
    "\n",
    "The Mel-scale is a scale of pitches judged by human listeners to be equal in distance one from another. It is roughly linear between 0 and 1000hz and logarithmic above 1000hz, as human ears become less adept at differentiating frequencies. We can think of Mel scale as a 'bin size' of frequencies to match how humans perceive speech. This is helpful when building spoken language systems because it means our feature representations more closely match what a human listener would perceive from the same audio. Here is a quick visualization of Mel scale:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6JUGfnbvRhTn"
   },
   "outputs": [],
   "source": [
    "fmax_dataset = 4096\n",
    "mel_basis = librosa.filters.mel(sr=sr_agent, n_fft=256, n_mels=128, fmax=fmax_dataset)\n",
    "\n",
    "plt.figure(figsize=(20, 5))\n",
    "librosa.display.specshow(mel_basis, x_axis=\"linear\", fmax=fmax_dataset)\n",
    "plt.title(\"Mel filter bank\", fontsize=20)\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4_arcTAI2DXy"
   },
   "source": [
    "We will use this helper function to visualize your merged audio file. Note the keywords used below. First we will set the max frequency considered for our mel scale (`fmax`) to the max possible frequency for this sampling rate (Nyquist Frequency, `sample_rate/2`). The number of bins is effectively the \"resolution\" in mel scale of the vertical axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "B5_ILd388jE2"
   },
   "outputs": [],
   "source": [
    "# helper function to plot a mel spectrogram\n",
    "# arguments: (wave array, sampling rate, number of mel bins, max frequency of mel scale)\n",
    "def plot_melspectrogram(\n",
    "    wav, sr, annotations=None, n_mels=256, fmax=4096, fig=None, ax=None, show_legend=True\n",
    "):\n",
    "    if ax == None:\n",
    "        fig, ax = plt.subplots(1, 1, figsize=(20, 5))\n",
    "    M = librosa.feature.melspectrogram(y=wav, sr=sr, n_mels=n_mels, fmax=fmax, n_fft=2048)\n",
    "    M_db = librosa.power_to_db(M, ref=np.max)\n",
    "    img = librosa.display.specshow(M_db, y_axis=\"mel\", x_axis=\"time\", ax=ax, fmax=fmax)\n",
    "    if show_legend:\n",
    "        ax.set(title=\"Mel spectrogram display\")\n",
    "        fig.colorbar(img, ax=ax, format=\"%+2.f dB\")\n",
    "\n",
    "    # iterate over list of text annotations and draw them\n",
    "    if annotations is not None:\n",
    "        for x, y, text in annotations:\n",
    "            ax.annotate(\n",
    "                text,\n",
    "                xy=(x, y),\n",
    "                xycoords=\"data\",\n",
    "                xytext=(10, -50),\n",
    "                textcoords=\"offset pixels\",\n",
    "                horizontalalignment=\"right\",\n",
    "                color=\"white\",\n",
    "                fontsize=20,\n",
    "                verticalalignment=\"bottom\",\n",
    "                arrowprops=dict(arrowstyle=\"-|>\", color=\"white\", lw=1, ls=\"-\"),\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yoG1OG8Q2WWX"
   },
   "source": [
    "Now we can use the helper function to visualize a Mel spectrogram of the combined audio file you created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JGdUPgQzMhTn"
   },
   "outputs": [],
   "source": [
    "plot_melspectrogram(merged_wav, sr_agent, annotations=None, n_mels=256, fmax=sr_agent / 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bmv6AiIQ8jE2"
   },
   "source": [
    "Notice there is no energy above 4kHz. The dataset was collected using telephone speech, which has a sampling rate of 8kHz. Therefore the highest meaningful frequency in these recordings is 4kHz. Use the plotting arguments below as your default when displaying audio to hand in. For convenience we also set these as the default for the helper function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v_qgH3t98jE2"
   },
   "outputs": [],
   "source": [
    "# we can adjust the keyword arguments\n",
    "fmax_dataset = 4096\n",
    "plot_melspectrogram(merged_wav, sr_agent, annotations=None, n_mels=256, fmax=fmax_dataset)\n",
    "plot_melspectrogram(merged_wav, sr_agent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pc0sJpZqOpdQ"
   },
   "source": [
    "### **Problem 2 (8 points): Silence removal** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8nNwW7rT8jE3"
   },
   "source": [
    "We often analyze utterances from a single speaker at a time, rather than the conversation. We also want to focus on the actual _speech_ audio rather than long silences / pauses between utterances.\n",
    "\n",
    "Focusing on the **agent**, your task is to remove most of the major silences from the raw waveform file. This will collapse all of the agent's utterances into a single array with just short pauses between utterances. Removing silences in this way helps focus on the speech parts of an audio file. Sometimes this process is called _voice activity detection_ (and is more dificult in scenarios with strong background noise or distortion when speech can be hard to identify from the background).\n",
    "\n",
    "One way to do this:\n",
    "\n",
    "1. Use `librosa.effects.split` to split the agent .wav file by silence.\n",
    "2. Clip out the silences and combine the audio back into a single array with silences removed. Ensure you aren't filtering too aggressively and clipping out actual audio.\n",
    "\n",
    "You are free to try other approaches to silence removal. It should sound like a more or less continuous utterance stream from the speaker, but it need not be perfect. Describe your approach briefly along with the implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "zI1UihzR8jE3"
   },
   "outputs": [],
   "source": [
    "recon_agent = None  # TODO: create a NumPy array with silences removed\n",
    "\n",
    "#############################\n",
    "#### YOUR CODE GOES HERE ####\n",
    "\n",
    "# Split the agent's audio by silence(which is defined by top_db)\n",
    "split_intervals = librosa.effects.split(wav_agent, top_db=60)\n",
    "\n",
    "# Concat intervals that contain speech\n",
    "recon_agent = np.concatenate([wav_agent[start:end] for start, end in split_intervals])\n",
    "\n",
    "#############################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kINn0sYO8jE3"
   },
   "outputs": [],
   "source": [
    "# listen to your audio file\n",
    "ipd.Audio(recon_agent, rate=sr_agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b3wIYvai8jE3"
   },
   "outputs": [],
   "source": [
    "# This should now have minimal gaps between utterances\n",
    "# (leaving some small silences is okay)\n",
    "plot_melspectrogram(recon_agent, sr_agent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J6M78UcCrz39"
   },
   "source": [
    "### **Problem 3 (8 points): Segment a single utterance**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_uypCvg68jE3"
   },
   "source": [
    "Finally, an entire audio signal is too long, so let's pick a few utterances for further work.\n",
    "\n",
    "In the conversation above, the agent says the following: `This is Harper Valley National Bank. My name is Elizabeth. How can I help you today?`. Slice a single wave array for just this part of the utterance and visualize the corresponding spectrogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "eunsx8m48jE3"
   },
   "outputs": [],
   "source": [
    "audio_signal = None  # TODO: represent as a NumPy array\n",
    "\n",
    "#############################\n",
    "#### YOUR CODE GOES HERE ####\n",
    "audio_signal = recon_agent[9000:120000]\n",
    "#############################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_GJJWYz78jE3"
   },
   "outputs": [],
   "source": [
    "# listen to your audio file\n",
    "ipd.Audio(audio_signal, rate=sr_agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LwunCr_R8jE4"
   },
   "outputs": [],
   "source": [
    "plot_melspectrogram(audio_signal, sr_agent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CpEozYpqud-7"
   },
   "source": [
    "### **Task 4 (10 points): Label words in the spectrogram** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tb1mACAB8jE4"
   },
   "source": [
    "We know the true _transcript_ for this utterance, but it's also important to build an understanding of how words and phonemes appear in a spectrogram representation. This assists when debugging spoken language systems, and especially when building text to speech engines.\n",
    "\n",
    "Using the helper function we created, you can add text labels to the spectrogram plot. Here is a single annotation to demonstrate the format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TFhUHNh9wV8W"
   },
   "outputs": [],
   "source": [
    "utt_words = [\n",
    "    (0.1, 1024, \"This\"),\n",
    "]\n",
    "plot_melspectrogram(audio_signal, sr_agent, annotations=utt_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D8Uw8MsawWLg"
   },
   "source": [
    "Your task is to label all remaining words in the utterance. Try to position each annotation at the center of each word in time (x axis). You may place the word anywhere along the frequency axis for readability. We won't check the time positions of your labels precisely, just point to each word without words overlapping when displayed.\n",
    "\n",
    "Position text anywhere along the frequency (y) axis to avoid text overlap, there is no \"correct\" answer on where to position words along this axis, only the time axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "T1N4alCF8jE4"
   },
   "outputs": [],
   "source": [
    "# TODO: Position the remaining words correctly using the same format:\n",
    "#  (time (s), frequency(Hz), annotation(string))\n",
    "utt_words = [\n",
    "    (0.1, 1024, \"This\"),\n",
    "]\n",
    "#############################\n",
    "#### YOUR CODE GOES HERE ####\n",
    "utt_words = [\n",
    "    (0.1, 1024, \"This\"),\n",
    "    (0.2, 768, \"is\"),\n",
    "    (0.6, 1024, \"Harper\"),\n",
    "    (1.0, 768, \"Valley\"),\n",
    "    (1.4, 1024, \"National\"),\n",
    "    (1.9, 768, \"Bank\"),\n",
    "    (2.5, 1024, \"My\"),\n",
    "    (2.7, 768, \"name\"),\n",
    "    (2.8, 1024, \"is\"),\n",
    "    (3.4, 768, \"Elizabeth\"),\n",
    "    (3.9, 1024, \"How\"),\n",
    "    (4.0, 768, \"can\"),\n",
    "    (4.1, 1024, \"I\"),\n",
    "    (4.3, 768, \"help\"),\n",
    "    (4.5, 1024, \"you\"),\n",
    "    (4.8, 768, \"today\"),\n",
    "]\n",
    "\n",
    "#############################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XSqgQr138jE4"
   },
   "outputs": [],
   "source": [
    "plot_melspectrogram(audio_signal, sr_agent, annotations=utt_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4M6ie7RQ337L"
   },
   "source": [
    "### **Task 5 (8 points): Visually estimate mean F_0**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hsHuj-Vc8jE4"
   },
   "source": [
    "F_0 is the fundamental frequency of a speaker (corresponding to the lowest harmonic of the speaker's glottal pulse train). On a Mel spectrogram, F_0 appears as the lowest frequency \"line\" of high energy in _voiced_ regions of speech (when the glottis is producing sound)\n",
    "\n",
    "Visually estimate the mean F_0 frequency during the word national. Does the F_0 curve have high variance during this word? (it's okay if your F_0 estimate isn't exact.)\n",
    "\n",
    "```\n",
    "According to the Mel spectrogram above, the mean F_0 frequency of the word 'national' is ~200Hz. Compared to others words like 'bank', 'name' and 'Elizabeth', the varience during 'national' is moderate.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NRsoTkPiDIFC"
   },
   "source": [
    "## IV. Visualizing alternative audio representations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aNOUvMOA8jE4"
   },
   "source": [
    "We've been visualizing our utterances in Mel Spectrograms because it is easier to \"read\" speech-relevant phones in this view. Now let's look at the raw waveform time series plot itself. We will use the same `audio_signal` waveform in all the exercises below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "s9qmhdyu8jE5"
   },
   "outputs": [],
   "source": [
    "def show_waveplot(audio_signal, sr, annotations=None):\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(20, 5))\n",
    "    # librosa.display.waveplot(audio_signal, sr=sr)\n",
    "    librosa.display.waveshow(audio_signal, sr=sr)\n",
    "    # iterate over list of text annotations and draw them\n",
    "    if annotations is not None:\n",
    "        for x, y, text in annotations:\n",
    "            ax.annotate(\n",
    "                text,\n",
    "                xy=(x, y),\n",
    "                xycoords=\"data\",\n",
    "                xytext=(10, -50),\n",
    "                textcoords=\"offset pixels\",\n",
    "                horizontalalignment=\"right\",\n",
    "                color=\"black\",\n",
    "                fontsize=20,\n",
    "                verticalalignment=\"bottom\",\n",
    "                arrowprops=dict(arrowstyle=\"-|>\", color=\"black\", lw=1, ls=\"-\"),\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CBIsPxnW8jE5"
   },
   "outputs": [],
   "source": [
    "show_waveplot(audio_signal, sr_agent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wy14mvcTENIa"
   },
   "source": [
    "### **Problem 6 (8 points): Label words in the time domain plot** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-W4YFXJn8jE5"
   },
   "source": [
    "Populate word-level annotations for the time domain plot. You should be able to use the same time annotations from the spectrogram plot. However you will need to adjust the y axis positions for words (again precise positions on the vertical axis are not graded, just try to prevent overlapping words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "4KfLWTe78jE5"
   },
   "outputs": [],
   "source": [
    "#  (time (s), amplitude, annotation(string))\n",
    "utt_words = [\n",
    "    (0.1, 0, \"This\"),\n",
    "]\n",
    "#############################\n",
    "#### YOUR CODE GOES HERE ####\n",
    "utt_words = [\n",
    "    (0.1, 0, \"This\"),\n",
    "    (0.2, 0.05, \"is\"),\n",
    "    (0.6, 0, \"Harper\"),\n",
    "    (1.0, 0.05, \"Valley\"),\n",
    "    (1.4, 0, \"National\"),\n",
    "    (1.9, 0.05, \"Bank\"),\n",
    "    (2.5, 0, \"My\"),\n",
    "    (2.7, 0.05, \"name\"),\n",
    "    (2.8, 0, \"is\"),\n",
    "    (3.4, 0.05, \"Elizabeth\"),\n",
    "    (3.9, 0, \"How\"),\n",
    "    (4.0, 0.05, \"can\"),\n",
    "    (4.1, 0, \"I\"),\n",
    "    (4.3, 0.05, \"help\"),\n",
    "    (4.5, 0, \"you\"),\n",
    "    (4.8, 0.05, \"today\"),\n",
    "]\n",
    "\n",
    "#############################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gaVhqOGi8jE5"
   },
   "outputs": [],
   "source": [
    "show_waveplot(audio_signal, sr_agent, utt_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XHM36lPT8jE5"
   },
   "source": [
    "Answer a few followup questions in text below:\n",
    "1. Describe what aspects of speech you can infer in this time series plot compared to the Mel spectrum above. Compare the two, when might looking at one vs the other help when working with audio data?\n",
    "2. Does each word have a distinct segment in the time-amplitude plot? (A segment in this context is portion of the waveform clearly larger than 0)\n",
    "3. Specify a word that has more than one distinct segment in the time-amplitude plot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mger-TzR0iGg"
   },
   "source": [
    "\n",
    "```\n",
    "1. The time series plot shows the amplitude of the audio signal over time, while the Mel spectrum show the frequency content of signal. \n",
    "When we need to analyze the exact timing of specific events or the overall shape of the waveform, it's better to look at the time series plot. \n",
    "When we need to recognize attributes of the speakers like gender or age, the Mel spectrogram is a better choice due to the frequency content of the signal.\n",
    "\n",
    "2. Not each word have a distinct segment.\n",
    "\n",
    "3. Elizabeth.\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNAUrda/9eNgghqbcYsqJgw",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
